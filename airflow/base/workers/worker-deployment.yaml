kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: airflow-worker
  labels:
    tier: airflow
    component: worker
    release: airflow
spec:
  serviceName: airflow-worker
  replicas: 1
  selector:
    matchLabels:
      tier: airflow
      component: worker
      release: airflow
  template:
    metadata:
      labels:
        tier: airflow
        component: worker
        release: airflow
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: worker
              topologyKey: kubernetes.io/hostname
            weight: 100
      terminationGracePeriodSeconds: 600
      restartPolicy: Always
      serviceAccountName: airflow-worker
      securityContext:
        runAsUser: 50000
        fsGroup: 65533
      initContainers:
        - name: wait-for-airflow-migrations
          image: apache/airflow:2.7.2
          imagePullPolicy: IfNotPresent
          args:          
            
            - python
            - -c
            - |
                  import airflow
                  import logging
                  import os
                  import time
          
                  from alembic.config import Config
                  from alembic.runtime.migration import MigrationContext
                  from alembic.script import ScriptDirectory
          
                  from airflow import settings
          
                  package_dir = os.path.abspath(os.path.dirname(airflow.__file__))
                  directory = os.path.join(package_dir, 'migrations')
                  config = Config(os.path.join(package_dir, 'alembic.ini'))
                  config.set_main_option('script_location', directory)
                  config.set_main_option('sqlalchemy.url', settings.SQL_ALCHEMY_CONN.replace('%', '%%'))
                  script_ = ScriptDirectory.from_config(config)
          
                  timeout=60
          
                  with settings.engine.connect() as connection:
                      context = MigrationContext.configure(connection)
                      ticker = 0
                      while True:
                          source_heads = set(script_.get_heads())
          
                          db_heads = set(context.get_current_heads())
                          if source_heads == db_heads:
                              break
          
                          if ticker >= timeout:
                              raise TimeoutError("There are still unapplied migrations after {} seconds.".format(ticker))
                          ticker += 1
                          time.sleep(1)
                          logging.info('Waiting for migrations... %s second(s)', ticker)
          envFrom:          
            []
          env:          
            - name: AIRFLOW_CONN_AWS
              valueFrom:
                secretKeyRef:
                  name: airflow-connections
                  key: AIRFLOW_CONN_AWS
            - name: AIRFLOW__CORE__FERNET_KEY
              valueFrom:
                secretKeyRef:
                  name: airflow-fernet-key
                  key: fernet-key
            - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
              valueFrom:
                configMapKeyRef:
                  name: db-connection
                  key: connection
            - name: AIRFLOW_CONN_AIRFLOW_DB
              valueFrom:
                configMapKeyRef:
                  name: db-connection
                  key: connection
            - name: AIRFLOW__WEBSERVER__SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: airflow-webserver-secret-key
                  key: webserver-secret-key
            - name: AIRFLOW__CELERY__RESULT_BACKEND
              valueFrom:
                secretKeyRef:
                  name: airflow-result-backend
                  key: connection
            - name: AIRFLOW__CELERY__BROKER_URL
              valueFrom:
                secretKeyRef:
                  name: airflow-broker-url
                  key: connection
      containers:
        - name: git-sync
          image: kennedyuche/airflow-git-sync:v1.0.0
          imagePullPolicy: IfNotPresent
          env:
            - name: GIT_TARGET_BRANCH
              value: "main"
            - name: GIT_REPO_URL
              value: "XXXX"
            - name: DAGS_TARGET_FOLDER
              value: "root"
          volumeMounts:
          - name: dags
            mountPath: /opt/airflow/dag

        - name: worker
          image: apache/airflow:2.7.2
          imagePullPolicy: IfNotPresent
          command: ["/bin/sh", "-c"]
          args:
            - |
              exec airflow celery worker
          ports:
            - name: worker-logs
              containerPort: 8793
          volumeMounts:
            - name: logs
              mountPath: "/opt/airflow/logs"
            - name: config
              mountPath: "/opt/airflow/airflow.cfg"
              subPath: airflow.cfg
              readOnly: true
            - name: config
              mountPath: "/opt/airflow/config/airflow_local_settings.py"
              subPath: airflow_local_settings.py
              readOnly: true
            - name: dags
              mountPath: /opt/airflow/dags
              readOnly: True
          env:
            - name: DUMB_INIT_SETSID
              value: "0"          
            - name: AIRFLOW_CONN_AWS
              valueFrom:
                secretKeyRef:
                  name: airflow-connections
                  key: AIRFLOW_CONN_AWS
            - name: AIRFLOW__CORE__FERNET_KEY
              valueFrom:
                secretKeyRef:
                  name: airflow-fernet-key
                  key: fernet-key
            - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
              valueFrom:
                configMapKeyRef:
                  name: db-connection
                  key: connection
            - name: AIRFLOW_CONN_AIRFLOW_DB
              valueFrom:
                configMapKeyRef:
                  name: db-connection
                  key: connection
            - name: AIRFLOW__WEBSERVER__SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: airflow-webserver-secret-key
                  key: webserver-secret-key
            - name: AIRFLOW__CELERY__RESULT_BACKEND
              valueFrom:
                secretKeyRef:
                  name: airflow-result-backend
                  key: connection
            - name: AIRFLOW__CELERY__BROKER_URL
              valueFrom:
                secretKeyRef:
                  name: airflow-broker-url
                  key: connection        
        - name: worker-log-groomer
          image: apache/airflow:2.7.2
          imagePullPolicy: IfNotPresent
          args: 
            - bash
            - /clean-logs
          
          env:
            - name: AIRFLOW__LOG_RETENTION_DAYS
              value: "15"
          resources:
            {}
          volumeMounts:
            - name: logs
              mountPath: "/opt/airflow/logs"
      volumes:
        - name: config
          configMap:
            name: airflow-config
        - name: dags
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: logs
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 30Gi